<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <meta name="title" content="ISO-Bench">
  <meta name="description" content="ISO-Bench benchmarks AI coding agents on real GPU inference optimization tasks from vLLM and SGLang, using both hard and soft metrics.">
  <meta name="keywords" content="ISO-Bench, coding agents, inference optimization, vLLM, SGLang, benchmark, LLM, GPU">

  <meta property="og:type" content="article">
  <meta property="og:title" content="ISO-Bench: Can Coding Agents Optimize Real-World Inference Workloads?">
  <meta property="og:description" content="A benchmark of 54 optimization tasks from vLLM and SGLang, evaluating coding agents with both hard and soft metrics.">
  <meta property="og:image" content="static/images/main_fig.png">

  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:title" content="ISO-Bench: Can Coding Agents Optimize Real-World Inference Workloads?">
  <meta name="twitter:description" content="A benchmark of 54 optimization tasks from vLLM and SGLang, evaluating coding agents with both hard and soft metrics.">
  <meta name="twitter:image" content="static/images/main_fig.png">

  <meta name="citation_title" content="ISO-Bench: Can Coding Agents Optimize Real-World Inference Workloads?">
  <meta name="citation_author" content="Ayush Nangia">
  <meta name="citation_author" content="Shikhar Mishra">
  <meta name="citation_author" content="Aman Gokrani">
  <meta name="citation_author" content="Paras Chopra">
  <meta name="citation_publication_date" content="2026">

  <title>ISO-Bench | Can Coding Agents Optimize Real-World Inference Workloads?</title>

  <link rel="icon" type="image/png" href="static/images/clawd_full.png">
  <link rel="apple-touch-icon" href="static/images/clawd_full.png">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet">

  <script defer src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script defer src="static/js/index.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.7/dist/chart.umd.min.js"></script>
</head>
<body>

  <button class="scroll-to-top" onclick="scrollToTop()" title="Scroll to top" aria-label="Scroll to top">
    <i class="fas fa-chevron-up"></i>
  </button>

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">ISO-Bench</h1>
            <p class="subtitle is-3">Can Coding Agents Optimize Real-World Inference Workloads?</p>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><a href="#">Ayush Nangia</a>,</span>
              <span class="author-block"><a href="#">Shikhar Mishra</a>,</span>
              <span class="author-block"><a href="#">Aman Gokrani</a>,</span>
              <span class="author-block"><a href="#">Paras Chopra</a></span>
            </div>

            <div class="is-size-6 publication-authors" style="margin-bottom: 1rem;">
              <span class="author-block"><strong>Pre-print 2026</strong></span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <span class="link-block">
                  <a href="#" target="_blank" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon"><i class="ai ai-arxiv"></i></span>
                    <span>arXiv</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="#" target="_blank" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon"><i class="fab fa-github"></i></span>
                    <span>Code</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="#" target="_blank" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">&#129303;</span>
                    <span>HuggingFace</span>
                  </a>
                </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>


  <!-- Main figure -->
  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <img src="static/images/main_fig.png" alt="ISO-Bench evaluation pipeline" style="width:100%; border-radius:16px; box-shadow: 0 10px 15px -3px rgb(0 0 0 / 0.1);">
        <h2 class="subtitle has-text-centered">
          <strong>Figure 1:</strong> ISO-Bench evaluation pipeline. Given a codebase and task description, a coding agent produces an optimization patch. We compare this patch against the human commit using hard metrics (TTFT, throughput) and soft metrics (bottleneck targeting, implementation approach).
        </h2>
      </div>
    </div>
  </section>


  <!-- Key Contributions -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-full">
          <h2 class="title is-3">Key Contributions</h2>
          <div class="content">
            <ul>
              <li>We introduce <strong>ISO-Bench</strong>, a benchmark of <strong>54 tasks</strong> curated from merged pull requests in <a href="https://github.com/vllm-project/vllm" target="_blank">vLLM</a> (39 tasks) and <a href="https://github.com/sgl-project/sglang" target="_blank">SGLang</a> (15 tasks), two of the most popular LLM serving frameworks.</li>
              <li>We propose a <strong>dual-metric evaluation framework</strong> combining hard (execution-based) and soft (LLM-based) metrics, revealing that hard metrics alone overestimate agent capabilities by up to <strong>20%</strong>.</li>
              <li>We introduce the <strong>quadrant framework</strong> that classifies agent outcomes into True Success, Good Intent, Lucky Win, and Failure, distinguishing genuine optimization from accidental improvements.</li>
              <li>We show that <strong>understanding &#8800; execution</strong>: agents demonstrate up to 87.2% correct bottleneck identification but achieve only 17.9% true success, exposing a fundamental capability gap.</li>
              <li>We evaluate <strong>three open-source models</strong> and find a <strong>0% success rate</strong>, highlighting the difficulty of real-world inference optimization.</li>
            </ul>
          </div>
        </div>
      </div>
    </div>
  </section>


  <!-- Benchmark Design -->
  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-full">
          <h2 class="title is-3">Benchmark Design</h2>
          <div class="content has-text-justified">
            <p>
              ISO-Bench tasks are curated from real merged pull requests that target performance bottlenecks in production LLM inference engines. Each task gives an agent a codebase snapshot and a bottleneck description from a GitHub issue. The agent has 120 minutes to produce an optimization patch, which is evaluated against the expert human solution from the corresponding PR.
            </p>
          </div>

          <div class="table-container">
            <table class="table is-striped is-fullwidth">
              <thead>
                <tr><th>Codebase</th><th>Tasks</th><th>Source</th><th>Optimization Targets</th></tr>
              </thead>
              <tbody>
                <tr>
                  <td><strong><a href="https://github.com/vllm-project/vllm" target="_blank">vLLM</a></strong></td>
                  <td>39</td>
                  <td>Merged PRs</td>
                  <td>Attention kernels, scheduling, memory management, batching</td>
                </tr>
                <tr>
                  <td><strong><a href="https://github.com/sgl-project/sglang" target="_blank">SGLang</a></strong></td>
                  <td>15</td>
                  <td>Merged PRs</td>
                  <td>Router logic, prefix caching, request handling, tokenization</td>
                </tr>
              </tbody>
              <tfoot>
                <tr><td><strong>Total</strong></td><td><strong>54</strong></td><td colspan="2">Each task includes: codebase snapshot + issue description + 120 min time limit</td></tr>
              </tfoot>
            </table>
          </div>

          <div class="content has-text-justified">
            <p>
              We evaluate four agent configurations: <strong>Claude Code</strong> (Claude Sonnet 4.5), <strong>Codex CLI</strong> (GPT-5), <strong>TRAE (Sonnet)</strong> (Claude Sonnet 4.5), and <strong>TRAE (GPT-5)</strong> (GPT-5). Each agent is given the full codebase, the bottleneck description, and 120 minutes to produce an optimization patch.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>


  <!-- The Dual-Metric Framework -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-full">
          <h2 class="title is-3">The Dual-Metric Framework</h2>

          <div class="table-container" style="margin-bottom: 1.5rem;">
            <table class="table is-fullwidth">
              <thead><tr><th style="width:25%;">Metric Type</th><th>What It Measures</th><th>How</th></tr></thead>
              <tbody>
                <tr>
                  <td><strong>Hard Metrics</strong></td>
                  <td>Execution outcomes - did the patch actually improve performance?</td>
                  <td>TTFT, throughput, latency benchmarks with a <strong>&ge;5%</strong> improvement threshold to filter GPU noise</td>
                </tr>
                <tr>
                  <td><strong>Soft Metrics</strong></td>
                  <td>Semantic understanding - did the agent target the right bottleneck?</td>
                  <td>LLM judge (Gemini-3-Flash-Preview) evaluates <em>bottleneck targeting</em> and <em>implementation approach</em></td>
                </tr>
              </tbody>
            </table>
          </div>

          <div class="content has-text-justified">
            <p>
              Combining both metrics yields four distinct outcomes. <strong>Q3 (Lucky Win)</strong> is the critical category that hard-only evaluation misses entirely - these agents improved metrics but targeted the wrong bottleneck. Their gains are coincidental and unlikely to generalize.
            </p>
          </div>

          <div class="has-text-centered" style="margin-bottom: 1.5rem;">
            <img src="static/images/quadrant_framework.png" alt="Quadrant framework diagram" class="framework-img">
          </div>
          <h2 class="subtitle has-text-centered" style="margin-bottom: 2rem;">
            <strong>Figure 2:</strong> The quadrant framework. Hard metrics on one axis, soft metrics on the other, yielding four distinct outcome categories.
          </h2>
        </div>
      </div>
    </div>
  </section>


  <!-- Experiments -->
  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-full">
          <h2 class="title is-3">Best Agent Achieves 46.2% True Success</h2>
          <div class="content has-text-justified">
            <p><strong>True Success</strong> requires both passing hard metrics <em>and</em> demonstrating correct bottleneck targeting via soft metrics. No single agent dominates: Claude Code leads on vLLM (46.2%) but drops to 26.7% on SGLang. Rankings flip completely between codebases.</p>
          </div>
          <div class="chart-container-wide">
            <canvas id="trueSuccessChart"></canvas>
          </div>

          <!-- Hard vs True Success -->
          <h3 class="title is-4">Hard Metrics Overestimate Agent Capabilities</h3>
          <div class="content has-text-justified">
            <p>The gap between Hard Success and True Success reveals how often agents succeed accidentally (Lucky Wins). Hard metrics alone can overestimate agent capabilities by up to 20%.</p>
          </div>

          <div class="columns is-mobile-stacked" style="margin-bottom: 1rem;">
            <div class="column">
              <h4 class="title is-5 has-text-centered">vLLM (39 tasks)</h4>
              <div class="chart-container">
                <canvas id="hardVsTrueVllmChart"></canvas>
              </div>
            </div>
            <div class="column">
              <h4 class="title is-5 has-text-centered">SGLang (15 tasks)</h4>
              <div class="chart-container">
                <canvas id="hardVsTrueSglangChart"></canvas>
              </div>
            </div>
          </div>

          <div class="content has-text-justified">
            <p>
              On vLLM, gaps range from 2.6% to 12.8%. On SGLang, Claude Code shows the largest gap (20.0%), while all other agents have zero gap. Notably, TRAE (Sonnet) and Claude Code use the same underlying model (Claude Sonnet 4.5), yet their scaffolding produces very different outcomes - scaffolding matters as much as the model.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>


  <!-- Understanding != Execution -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-full">
          <h2 class="title is-3">Understanding &#8800; Execution</h2>
          <div class="content has-text-justified">
            <p>
              Agents frequently identify the correct bottleneck but fail to implement a working solution. On vLLM, three of four agents have their highest quadrant count in Q2 (Good Intent, Bad Execution): <strong>knowing what to do is not the same as doing it correctly.</strong>
            </p>
          </div>

          <div class="notification is-warning is-light has-text-centered callout-stat">
            <p class="callout-number has-text-weight-bold has-text-danger">69.3%</p>
            <p class="callout-description">TRAE (GPT-5) correctly identifies 87.2% of vLLM bottlenecks but achieves only 17.9% true success - the largest gap of any agent.</p>
          </div>

          <!-- Good Intent figures from paper -->
          <div class="columns is-mobile-stacked" style="margin-bottom: 1.5rem;">
            <div class="column">
              <div class="has-text-centered">
                <img src="static/images/good_intent_vllm.png" alt="Good Intent vs Bad Execution on vLLM" class="figure-img">
              </div>
              <p class="has-text-centered figure-caption"><strong>Figure 3:</strong> Good Intent vs Bad Execution on vLLM (39 tasks). Light bars show correct target identification (Q1+Q2). Dark bars show True Success (Q1).</p>
            </div>
            <div class="column">
              <div class="has-text-centered">
                <img src="static/images/good_intent_sglang.png" alt="Good Intent vs Bad Execution on SGLang" class="figure-img">
              </div>
              <p class="has-text-centered figure-caption"><strong>Figure 4:</strong> Good Intent vs Bad Execution on SGLang (15 tasks). Light bars show correct target identification (Q1+Q2). Dark bars show True Success (Q1).</p>
            </div>
          </div>

          <div class="content has-text-justified">
            <p>
              Even the best agent (Claude Code) has a 38.4% gap on vLLM. On SGLang, the gap narrows: all agents except Claude Code identify the correct target in all 15 tasks and show much stronger execution. The bottleneck is not comprehension but <em>implementation</em>.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>


  <!-- Quadrant Distribution -->
  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-full">
          <h2 class="title is-3">Most Failures Are Good Intent, Bad Execution</h2>
          <div class="content has-text-justified">
            <p>The full quadrant distribution reveals where each agent spends most of its attempts.</p>
          </div>

          <div class="columns is-mobile-stacked" style="margin-bottom: 1rem;">
            <div class="column">
              <h4 class="title is-5 has-text-centered">vLLM (39 tasks)</h4>
              <div class="chart-container">
                <canvas id="quadrantVllmChart"></canvas>
              </div>
            </div>
            <div class="column">
              <h4 class="title is-5 has-text-centered">SGLang (15 tasks)</h4>
              <div class="chart-container">
                <canvas id="quadrantSglangChart"></canvas>
              </div>
            </div>
          </div>

          <div class="content has-text-justified">
            <p>
              On vLLM, Q2 (Good Intent) dominates for most agents - the primary failure mode is execution, not understanding. On SGLang, most outcomes are Q1 (True Success), except for Claude Code which still struggles with execution despite using the same model as TRAE (Sonnet).
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>


  <!-- Performance Does Not Generalize -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-full">
          <h2 class="title is-3">Performance Does Not Generalize Across Codebases</h2>
          <div class="content has-text-justified">
            <p>
              Agent rankings flip between vLLM and SGLang. Each agent maintains its preferred strategy regardless of the project - Claude Code favors alternative approaches (succeeds on vLLM, fails on SGLang), while TRAE agents match the reference approach (succeeds on SGLang, fails on vLLM).
            </p>
          </div>

          <div class="columns is-mobile-stacked" style="margin-bottom: 1.5rem;">
            <div class="column">
              <div class="has-text-centered">
                <img src="static/images/approach_vllm.png" alt="Approach distribution on vLLM" class="figure-img">
              </div>
              <p class="has-text-centered figure-caption"><strong>Figure 5:</strong> Approach distribution on vLLM (39 tasks).</p>
            </div>
            <div class="column">
              <div class="has-text-centered">
                <img src="static/images/approach_sglang.png" alt="Approach distribution on SGLang" class="figure-img">
              </div>
              <p class="has-text-centered figure-caption"><strong>Figure 6:</strong> Approach distribution on SGLang (15 tasks).</p>
            </div>
          </div>

          <div class="content has-text-justified">
            <p>
              TRAE (Sonnet) and Claude Code both use <strong>Claude Sonnet 4.5</strong>, yet differ substantially: Claude Code achieves 46.2% on vLLM vs. 26.7% on SGLang, while TRAE (Sonnet) reaches 28.2% on vLLM vs. 80.0% on SGLang. The model is identical - scaffolding determines outcomes. Single-codebase evaluations overstate generalization.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>


  <!-- Open-Source Models -->
  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-full">
          <h2 class="title is-3">Open-Source Models Achieve 0% Success</h2>
          <div class="content has-text-justified">
            <p>
              We evaluated three open-source models using the same TRAE-Agent scaffolding. None produced a single working optimization. Their failures cluster into distinct patterns:
            </p>

            <h4>MiniMax-M2.1 - <em>Planning Without Execution</em></h4>
            <p>
              Verbalized detailed optimization plans for 75 steps but never executed a single tool call. Generated 81,782 output tokens over 477 seconds with zero tool invocations.
            </p>
            <div class="table-container">
              <table class="table is-fullwidth" style="font-size: 0.9rem;">
                <tbody>
                  <tr><td><strong>Tool calls</strong></td><td>0</td></tr>
                  <tr><td><strong>Steps</strong></td><td>75</td></tr>
                  <tr><td><strong>Duration</strong></td><td>477s</td></tr>
                  <tr><td><strong>Output tokens</strong></td><td>81,782 (171 tok/s)</td></tr>
                  <tr><td><strong>Input tokens</strong></td><td>1,599,945</td></tr>
                </tbody>
              </table>
            </div>
            <pre class="code-block"><code>Step 73: Completed
LLM Response: The user keeps saying I haven't completed
    the task. I need to explore the repository and
    make changes. Let me start by using the
    str_replace_based_edit_tool tool.

    I need to actually use the tools now.
    I need to actually use the tools now.
    I need to actually use the tools now.
    I need to actually use the tools now.
    I need to actually use the tools now.
    <span style="color: #f5576c;">[...repeated 2,412 times total...]</span>

Step 74: Completed
LLM Response: I need to actually use the tools now...</code></pre>
            <p>
              The phrase "I need to actually use the tools now" appears 2,412 times without a single invocation.
            </p>

            <h4>GPT-OSS-120B - <em>Environment Confusion</em></h4>
            <p>
              Instead of optimizing vLLM code, attempted to create mock implementations of PyTorch, Triton, and Transformers libraries inside the project directory (~84 file creation attempts).
            </p>
            <pre class="code-block"><code><span style="color: #6a9955;">Files the model attempted to create:</span>
<span style="color: #4ec9b0;">+ vllm_core-0006/torch/__init__.py</span>
<span style="color: #4ec9b0;">+ vllm_core-0006/torch/nn/__init__.py</span>
<span style="color: #4ec9b0;">+ vllm_core-0006/torch/cuda/__init__.py</span>
<span style="color: #4ec9b0;">+ vllm_core-0006/triton/__init__.py</span>
<span style="color: #4ec9b0;">+ vllm_core-0006/transformers/__init__.py</span>

<span style="color: #6a9955;">Contents of attempted torch/__init__.py:</span>
class dtype:
    pass

float16 = 'float16'
float32 = 'float32'
__version__ = '0.0.0'</code></pre>
            <p>
              The model cannot distinguish between "code I should optimize" and "libraries I should use."
            </p>

            <h4>GLM-4.7 - <em>Task Completion Failure</em></h4>
            <p>
              Made valid code edits (59 successful <code>str_replace</code> operations across 386 tool calls) but failed to complete the task workflow.
            </p>
            <div class="table-container">
              <table class="table is-fullwidth" style="font-size: 0.9rem;">
                <tbody>
                  <tr><td><strong>Tool calls</strong></td><td>386 (327 bash, 59 str_replace)</td></tr>
                  <tr><td><strong>Final status</strong></td><td>max_steps_exceeded (400 steps)</td></tr>
                </tbody>
              </table>
            </div>
            <pre class="code-block"><code><span style="color: #6a9955;">Successful edits committed (Step 196):</span>
git commit output: 2 files changed, 11 insertions(+), 9 deletions(-)

<span style="color: #6a9955;">Sample optimization in scheduler.py:</span>
<span style="color: #f5576c;">- self._num_batched_tokens += num_batched_tokens</span>
<span style="color: #4ec9b0;">+ self._num_batched_tokens = self._num_batched_tokens + num_batched_tokens</span>

<span style="color: #6a9955;">Then at Step 198, attempted to verify:</span>
<span style="color: #f5576c;">error: patch failed: tests/core/test_scheduler.py:214</span>
<span style="color: #f5576c;">error: tests/core/test_scheduler.py: patch does not apply</span>

Model response: "Let me try a different approach..."
<span style="color: #f5576c;">[...cycled through git operations for 200+ more steps...]</span>

Final status: max_steps_exceeded (400 steps)</code></pre>
            <p>
              The model committed valid edits but couldn't interpret "patch does not apply" (already applied) as success, cycling for 200+ steps without calling <code>finish</code>.
            </p>

            <p>
              These failures illustrate three qualitatively different challenges: tool use capability (MiniMax), environment grounding (GPT-OSS), and workflow management (GLM). Real-world inference optimization requires all three.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>


  <!-- BibTeX -->
  <section class="section hero is-light" id="BibTeX">
    <div class="container is-max-desktop content">
      <div class="bibtex-header">
        <h2 class="title">BibTeX</h2>
        <button class="copy-bibtex-btn" onclick="copyBibTeX()" title="Copy BibTeX to clipboard">
          <i class="fas fa-copy"></i>
          <span class="copy-text">Copy</span>
        </button>
      </div>
      <pre id="bibtex-code"><code>@article{isobench2026,
  title   = {ISO-Bench: Can Coding Agents Optimize
             Real-World Inference Workloads?},
  author  = {Ayush Nangia and Shikhar Mishra and Aman Gokrani and Paras Chopra},
  year    = {2026}
}</code></pre>
    </div>
  </section>


  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
              You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

  <!-- Chart.js Initialization -->
  <script>
  document.addEventListener('DOMContentLoaded', function() {
    const agents = ['Claude Code', 'Codex CLI', 'TRAE (Sonnet)', 'TRAE (GPT-5)'];

    // Color palette
    const colors = {
      vllm: '#4e79a7',
      sglang: '#e15759',
      hard: '#a0cbe8',
      true: '#4e79a7',
      q1: '#59a14f',
      q2: '#edc948',
      q3: '#f28e2b',
      q4: '#e15759'
    };

    // Shared chart options
    const defaultOptions = {
      responsive: true,
      maintainAspectRatio: true,
      plugins: {
        legend: { position: 'top', labels: { font: { family: 'Inter, sans-serif', size: 11 }, boxWidth: 14, padding: 12 } },
        tooltip: { callbacks: { label: function(ctx) { return ctx.dataset.label + ': ' + ctx.raw + (ctx.dataset.isCount ? '' : '%'); } } }
      },
      scales: {
        y: { beginAtZero: true, ticks: { font: { family: 'Inter, sans-serif', size: 11 } } },
        x: { ticks: { font: { family: 'Inter, sans-serif', size: 10 }, maxRotation: 45, minRotation: 0 } }
      }
    };

    // 1. True Success Rates
    new Chart(document.getElementById('trueSuccessChart'), {
      type: 'bar',
      data: {
        labels: agents,
        datasets: [
          { label: 'vLLM (39 tasks)', data: [46.2, 20.5, 28.2, 17.9], backgroundColor: colors.vllm, borderRadius: 4 },
          { label: 'SGLang (15 tasks)', data: [26.7, 80.0, 80.0, 86.7], backgroundColor: colors.sglang, borderRadius: 4 }
        ]
      },
      options: {
        ...defaultOptions,
        plugins: {
          ...defaultOptions.plugins,
          title: { display: true, text: 'True Success Rate (%)', font: { family: 'Inter, sans-serif', size: 14, weight: '600' } }
        },
        scales: {
          ...defaultOptions.scales,
          y: { ...defaultOptions.scales.y, max: 100, ticks: { ...defaultOptions.scales.y.ticks, callback: v => v + '%' } }
        }
      }
    });

    // 2. Hard vs True Success - vLLM
    new Chart(document.getElementById('hardVsTrueVllmChart'), {
      type: 'bar',
      data: {
        labels: agents,
        datasets: [
          { label: 'Hard Success', data: [56.4, 33.3, 33.3, 20.5], backgroundColor: colors.hard, borderRadius: 4 },
          { label: 'True Success', data: [46.2, 20.5, 28.2, 17.9], backgroundColor: colors.true, borderRadius: 4 }
        ]
      },
      options: {
        ...defaultOptions,
        scales: {
          ...defaultOptions.scales,
          y: { ...defaultOptions.scales.y, max: 70, ticks: { ...defaultOptions.scales.y.ticks, callback: v => v + '%' } }
        }
      }
    });

    // 3. Hard vs True Success - SGLang
    new Chart(document.getElementById('hardVsTrueSglangChart'), {
      type: 'bar',
      data: {
        labels: agents,
        datasets: [
          { label: 'Hard Success', data: [46.7, 80.0, 80.0, 86.7], backgroundColor: colors.hard, borderRadius: 4 },
          { label: 'True Success', data: [26.7, 80.0, 80.0, 86.7], backgroundColor: colors.true, borderRadius: 4 }
        ]
      },
      options: {
        ...defaultOptions,
        scales: {
          ...defaultOptions.scales,
          y: { ...defaultOptions.scales.y, max: 100, ticks: { ...defaultOptions.scales.y.ticks, callback: v => v + '%' } }
        }
      }
    });

    // 4. Quadrant Distribution - vLLM (stacked)
    new Chart(document.getElementById('quadrantVllmChart'), {
      type: 'bar',
      data: {
        labels: agents,
        datasets: [
          { label: 'Q1 True Success', data: [18, 8, 11, 7], backgroundColor: colors.q1, isCount: true, borderRadius: 2 },
          { label: 'Q2 Good Intent', data: [15, 20, 20, 27], backgroundColor: colors.q2, isCount: true, borderRadius: 2 },
          { label: 'Q3 Lucky Win', data: [4, 5, 2, 1], backgroundColor: colors.q3, isCount: true, borderRadius: 2 },
          { label: 'Q4 Failure', data: [2, 6, 6, 4], backgroundColor: colors.q4, isCount: true, borderRadius: 2 }
        ]
      },
      options: {
        ...defaultOptions,
        plugins: {
          ...defaultOptions.plugins,
          legend: { ...defaultOptions.plugins.legend, labels: { ...defaultOptions.plugins.legend.labels, boxWidth: 12 } }
        },
        scales: {
          ...defaultOptions.scales,
          x: { ...defaultOptions.scales.x, stacked: true },
          y: { ...defaultOptions.scales.y, stacked: true, max: 39, ticks: { ...defaultOptions.scales.y.ticks } }
        }
      }
    });

    // 5. Quadrant Distribution - SGLang (stacked)
    new Chart(document.getElementById('quadrantSglangChart'), {
      type: 'bar',
      data: {
        labels: agents,
        datasets: [
          { label: 'Q1 True Success', data: [4, 12, 12, 13], backgroundColor: colors.q1, isCount: true, borderRadius: 2 },
          { label: 'Q2 Good Intent', data: [8, 3, 3, 2], backgroundColor: colors.q2, isCount: true, borderRadius: 2 },
          { label: 'Q3 Lucky Win', data: [3, 0, 0, 0], backgroundColor: colors.q3, isCount: true, borderRadius: 2 },
          { label: 'Q4 Failure', data: [0, 0, 0, 0], backgroundColor: colors.q4, isCount: true, borderRadius: 2 }
        ]
      },
      options: {
        ...defaultOptions,
        plugins: {
          ...defaultOptions.plugins,
          legend: { ...defaultOptions.plugins.legend, labels: { ...defaultOptions.plugins.legend.labels, boxWidth: 12 } }
        },
        scales: {
          ...defaultOptions.scales,
          x: { ...defaultOptions.scales.x, stacked: true },
          y: { ...defaultOptions.scales.y, stacked: true, max: 15, ticks: { ...defaultOptions.scales.y.ticks } }
        }
      }
    });
  });
  </script>

</body>
</html>
